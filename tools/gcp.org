* GCP
* GCP Spark Notebook
** First time setup
*** Enable APIs
*** Create a bucket for code
  - https://console.cloud.google.com/storage/browser
  - Notebooks will be stored in /notebooks/jupyter in the bucket
*** Create a cluster
  - Enable component gateway
  - Add Anaconda and Jupyter
  - Specify the bucket created above
*** Open Jupyter
  - Browse to Web Interfaces tab of the cluster
  - Launch Jupyter
  - Party...
** Cluster definitions
*** Set project
gcloud config set project PROJECT_NAME
*** Light cluster
**** Command line v1 Spark 2.3
gcloud beta dataproc clusters create light-cluster --enable-component-gateway --bucket spark-playground-bucket --subnet default --zone us-west1-b --master-machine-type n1-standard-4 --master-boot-disk-size 500 --num-workers 2 --worker-machine-type n1-standard-1 --worker-boot-disk-size 500 --num-preemptible-workers 2 --image-version 1.3-deb9 --optional-components ANACONDA,JUPYTER --labels env=ENV,role=data-exploration,project=PROJECT,ticket=JIRA-TICKET --project PROJECT_NAME
  - WARNING: For PD-Standard without local SSDs, we strongly recommend
    provisioning 1TB or larger to ensure consistently high I/O performance.
    See https://cloud.google.com/compute/docs/disks/performance for
    information on disk I/O performance.
**** Command line v2 Spark 2.4
gcloud beta dataproc clusters create light-spark --enable-component-gateway --bucket spark-playground-bucket --subnet default --zone us-west1-b --master-machine-type n1-standard-4 --master-boot-disk-size 1000 --num-workers 2 --worker-machine-type n1-standard-2 --worker-boot-disk-size 500 --num-preemptible-workers 2 --image-version 1.4-debian9 --optional-components ANACONDA,JUPYTER,ZEPPELIN --labels env=ENV,role=data-exploration,project=PROJECT,ticket=JIRA-TICKET --properties spark-env:spark.jars.packages=org.apache.spark:spark-avro_2.11:2.4.3 --project PROJECT_NAME
*** SSH to cluster master
gcloud beta compute --project "PROJECT_NAME" ssh --zone "us-west1-b" "light-spark-m"
*** edit spark conf
sudo vim $SPARK_HOME/conf/spark-defaults.conf
Trying:
spark.jars.packages=org.apache.spark:spark-avro_2.11:2.4.3

** Notebook setup
*** Apache Zeppelin
**** Edit interpreter for spark and add to dependencies:
org.apache.spark:spark-avro_2.11:2.4.3
**** github setup
  - /etc/zeppelin/conf/interpreter.json

Inside of:

 "interpreterGroup": [
  {
  "name": "spark",

      "dependencies": [
        {
          "groupArtifactVersion": "org.apache.spark:spark-avro_2.11:2.4.3",
          "local": false
        }
      ],

  - /etc/zeppelin/conf/zeppelin-site.xml

Replace:
  <property>
    <name>zeppelin.notebook.storage</name>
    <value>org.apache.zeppelin.notebook.repo.GitHubNotebookRepo</value>
    <description>
      versioned notebook persistence layer implementation
    </description>
  </property>


Add:

<property>
  <name>zeppelin.notebook.git.remote.url</name>
  <value>GITHUB_URL</value>
  <description>remote Git repository URL</description>
</property>

<property>
  <name>zeppelin.notebook.git.remote.username</name>
  <value>GITHUB_USERNAME</value>
  <description>remote Git repository username</description>
</property>

<property>
  <name>zeppelin.notebook.git.remote.access-token</name>
  <value>TOKEN</value>
  <description></description>
</property>

<property>
  <name>zeppelin.notebook.git.remote.origin</name>
  <value>origin</value>
  <description>Git repository remote</description>
</property>
*** SSH Tunnel to web interfaces

  - Open an SSH tunnel
  gcloud compute ssh light-spark-m \
    --project=PROJECT \
    --zone=us-west1-b -- -D 1080 -N
  - Start an instance of Chrome with the proxy set

  "/Applications/Google Chrome.app/Contents/MacOS/Google Chrome" \
    --proxy-server="socks5://localhost:1080" \
    --user-data-dir="/tmp/light-spark-m" http://light-spark-m:18080
